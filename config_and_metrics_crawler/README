The crawler is a reference implementation of a data collector for the CloudSight service,
contributed by the Origami project. It is meant for experimental purposes only.

CRAWLERHOST is the IP address of the machine on which the crawler runs.

Usage:
------

    python crawler.py [-h] [--url URL] [--namespace NAMESPACE] [--features FEATURES] [--since SINCE] [--frequency FREQUENCY] [--options OPTIONS]

This will emit or more "snapshots" of the CRAWLERHOST. Each snapshot emits a "frame", which
is a CSV file containing one feature per line, in the format: <type> TAB <key> TAB <value>.

  -h, --help             show this help message and exit

  --url URL              Send (push) the snapshot data to URL. Defaults to file://frame.
  
  --namespace NAMESPACE  Data source URI for this crawler. This must be a / separated string
                         starting with a leading / (e.g. /myapp/dbcluster/dbprimary).
                         Defaults to /CRAWLERHOST.
  						 
  --features FEATURES	 What feature types to crawl, specified as a comma-separated list of one or
                         more of the feature types: os,disk,process,connection,metric,package,file,config.
                         For each feature type you can optionally configure the crawler using --options.
                         Defaults to 'os,disk,process,connection'.
  
  --since SINCE	         Only emit features created/modified/accessed since a given time. SINCE must be
                         one of { EPOCH, BOOT, LASTSNAPSHOT }. EPOCH is the epoch time for the host OS
                         platform, BOOT is the last boot time, LASTSNAPSHOT is the last snapshot time 
                         (only relevant with the --frequency option). when --since LASTSNAPSHOT is used 
                         with --frequency FREQUENCY, a sequence of snapshots are emitted, with the first 
                         snapshot using --since BOOT and the subsequent ones using --since LASTSNAPSHOT.
                         Defaults to BOOT.
  
  --frequency FREQUENCY  Interval in secs between successive snapshots. Defaults to -1.
  
  --compress COMPRESS	 Whether or not to GZIP compress the emitted frame. COMPRESS must be one of
                         { true, false }. Defaults to true.

  --logfile LOGFILE	     File in which to write the crawler log. Defaults to crawler.log. 
  
  --options OPTIONS      JSON dict of crawler options for each feature type you provided via --features.
                         Only "file" and "config" features accept options. They defaults to:
                         '{
                            "file": { 
                               "root_dir": "/", 
                               "exclude_dirs": ["boot","dev","proc","sys","mnt","tmp"]
                            },
                            "config": { 
                               "root_dir": "/",
                               "known_config_files": ["etc/passwd","etc/hosts","etc/issue","etc/mtab","etc/group"],
                               "discover_config_files": true
                            }
                          }'


Starting the crawler in PULL (network) mode
-------------------------------------------

All arguments shown above are optional. If no argument is given, the crawler will start in "listen" mode:

	python crawler.py &

    > Crawler is listening for commands on port 9999

This allows you to drive the crawler over a network to pull feature data, using one of these REST APIs
(all APIs require a http://CRAWLERHOST:9999 prefix):

1. /getsnapshot[?features=FEATURES][&since=SINCE]

   Returns a snapshot of FEATURES touched since SINCE as a text/csv stream.
   Example: /getsnapshot?features=process,connection

2. /snapshot[?url=URL][&namespace=NAMESPACE][&features=FEATURES][&since=SINCE][&frequency=FREQUENCY][&compress=COMPRESS]

   Emits a snapshot of FEATURES to URL every FREQUENCY seconds.
   Example: /snapshot?url=http://foo/bar&features=process,connection,metric&frequency=60

3. /status

   Returns a JSON dictionary of active snapshot tasks

4. /abort/ID
   
   Attempts to terminate snapshot task ID

5. /help
   
   Show this API help


The options URL, NAMESPACE, FEATURES, SINCE, FREQUENCY, COMPRESS and OPTIONS are as defined in the Usage section above.


Starting the crawler in PUSH (autonomous) mode
----------------------------------------------

You can also invoke the crawler from the command line with at least one optional argument, to have it run in
autonomous mode. One way to do this is to write your own wrapper program that reads the crawler arguments from a
config file, and starts the crawler as a service (e.g. via init.d).


Customizing the crawler
-----------------------

Each snapshot emits a "frame" containing a list of "features" that describe various attributes of the host machine's state 
at the time of the snapshot. Each feature contains 3 fields: feature-type, feature-key, feature-value. The feature-type is one of
"os", "metric", "disk", "package", "connection", "file", "config" - each representing a particular type of data extracted
from the host machine. The feature-value is a JSON object whose attributes are defined by the feature-type. The feature-key
is a unique feature identifier (you can think of it as a hash of the feature-value).

Some feature-types can be configured to limit the crawler to only extracting a subset of the instances of that feature-type
from the host machine. Take the "file" feature for example. Its default configuration is:
     "file" : {"root_dir": "/", "exclude_dirs":["boot","dev","proc","sys","mnt","tmp"]}
which means crawl the file system tree starting at the root directory "/", and exclude the specified sub-directories of
this root directory. You can alter this defaul configuration using the --options flag.

Some feature-types cannot be configured. For example the "process" feature has no configuration options.

You can turn off a feature-type entirely by simply not listing it in --features FEATURES. For example:
     --features 'process,connection'
will only emit "process" and "connection" feature types into the frame.


Extending the crawler
---------------------

You may extend the crawler to extract new feature types of interest to you (e.g. a log stream, a JVM heap dump, etc).
This is easy to do: just create a new feature type, and add your code to /crawler/crawlutils.py following the template
used by the other feature crawlers already there.

You can also extend the crawler to replace its config_file_discovery heuristic with you own. This is the heuristic
used to discover unknown configuration files - a capability that you can turn on by setting the "discover_config_files"
option for the "config" feature-type to true. To replace the default heuristic, look in /crawler/crawlutils.py where
it creates an instance of the Crawler class inside the snapshot() method like this: 
    crawler = Crawler(feature_epoch=since_when)
Replace this with something like this:
    crawler = crawlutils.Crawler(feature_epoch=since_when, config_discovery_heuristic=myDiscoveryFunction)
where "myDiscoveryFunction" is your heuristic function. It should have this signature:
    def myDiscoveryFunction(file_path)
where file_path is a fully qualified path name to a file. Every time the config crawler examines a file,
it will now invoke your function and pass it the file's path name. Your function simply needs to return True
if it determines this is a configuration file, and False if not.


    
    
        

