#!/usr/bin/env python

from __future__ import print_function
import logging
import logging.handlers
import time
import signal
import sys
import argparse
import datetime
import csv
import os
import  elasticsearch 
from cStringIO import StringIO
import elastic_index
import resource

try:
    import simplejson as json
except:
    import json
    
import timeout
import kafka as kafka_python
import pykafka
from pykafka.exceptions import SocketDisconnectedError, ZookeeperConnectionLost

import frame_annotator_2
import usndb

logger_file = "/var/log/cloudsight/vulnerability_annotator.log"
PROCESSOR_GROUP = "vuln_annotator"
RETRY_INTERVAL = 60 + 5 # zookeper timeout 60 sec + 5 more

class NullHandler(logging.Handler):
    def emit(self, record):
        pass


def get_memory_usage():
    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024

class KafkaInterface(object):
    def __init__(self, kafka_url, kafka_zookeeper_port, logger, receive_topic, publish_topic, notify_topic):

        '''
        XXX autocreate topic doesn't work in pykafka, so let's use kafka-python
        to create one.
        '''

        try_num = 1
        while True:
            try:
                kafka_python_client = kafka_python.KafkaClient(kafka_url)
                kafka_python_client.ensure_topic_exists(receive_topic)
                kafka_python_client.ensure_topic_exists(publish_topic)
                kafka_python_client.ensure_topic_exists(notify_topic)
                break
            except pykafka.exceptions.UnknownError, e:
                logger.info('try_num={}, error connecting to {} , reason={}'.format(try_num, kafka_url, str(e)))
                time.sleep(60)
                try_num = try_num + 1

        self.logger = logger
        self.kafka_url = kafka_url
        kafka = pykafka.KafkaClient(hosts=kafka_url)
        self.receive_topic_object = kafka.topics[receive_topic]
        self.publish_topic_object = kafka.topics[publish_topic]
        self.notify_topic_object = kafka.topics[notify_topic]

        # XXX replace the port in the broker url. This should be passed.
        zk_url = kafka_url.split(":")[0] + ":" + kafka_zookeeper_port

        self.consumer = self.receive_topic_object.get_balanced_consumer(
                                 reset_offset_on_start=True,          
                                 # ideally, we want to pickup where we left off, I encountered
                                 # a bug where pykafka unpack_from method throws error saying the message size
                                 # exceeds 512 MB specified below; no single message in any channel exceeds 10 MB
                                 # for this reason I guess I suspect that pykafka is trying to download all outstanding
                                 # messages in a parititon in one request; in my observations the instance that processed
                                 # a different set of partitions did not have any problem. For this resaon,
                                 # I limited the queue size to max 10; As a result of this setting between restarts
                                 # of vulnerability annotators we might skip number of messages unprocessed;
                                 # we now have a rescan job that will take care of skipped offsets
                                 fetch_message_max_bytes=512*1024*1024,
                                 queued_max_messages=10,
                                 consumer_group=PROCESSOR_GROUP,
                                 auto_offset_reset=pykafka.common.OffsetType.LATEST,
                                 auto_commit_enable=True,
                                 zookeeper_connect = zk_url)
        self.producer = self.publish_topic_object.get_producer()
        self.notifier = self.notify_topic_object.get_producer()

    def next_frame(self):
        message = self.consumer.consume() 
        if message is not None:
            self.logger.info('fetched offset: partition={}, offset={}'.format(message.partition_id, message.offset))
            return message.value

    def stop(self):
        self.consumer.stop()

    @timeout.timeout(2)
    def publish(self, data, uuid):
        trial_num=0
        while True:
            try:
                self.producer.produce(data)
                break
            except timeout.TimeoutError, e:
                self.logger.warn('Could not send data to {0}, uuid={1}, trial={2} reason={3}, data={4}'.format(self.kafka_url, uuid, trial_num, e, data))
                trial_num = trial_num + 1
            

    @timeout.timeout(2)
    def notify(self, data, uuid):
        trial_num=0
        while True:
            try:
                self.notifier.produce(data)
                break
            except timeout.TimeoutError, e:
                self.logger.warn('Could not send data to {0}, uuid={1}, trial={2} reason={3}, data={4}'.format(self.kafka_url, uuid, trial_num, e, data))
                trial_num = trial_num + 1


def sigterm_handler(signum=None, frame=None):
    print ('Received SIGTERM signal. Goodbye!', file=sys.stderr)
    sys.exit(0)

signal.signal(signal.SIGTERM, sigterm_handler)

count = 0 # number of frames processed
load_count = 0
    

def get_client(args, logger):
    max_retry_count=10
    retry_count = 0

    while retry_count < max_retry_count:
        retry_count += 1
        try:
            return KafkaInterface(args.kafka_url, args.kafka_zookeeper_port, logger, args.receive_topic, args.annotation_topic, args.notification_topic)
        except Exception as e:
            logger.exception(e)
            logger.info("retrying after 10 sec")
            time.sleep(10)
    logger.info("Max retries for get_client exceeded. Exiting.")
    sys.exit(1)

def process_message(args, logger):
    
    global count, load_count

    annotator = frame_annotator_2.MakeScan()
    last_usn_load_time = 0
    usn_info_list, distro_pkg_usn_map = None, None
    client = None

    last_mem_size = get_memory_usage()
    while True:
        if client is None:
            client = get_client(args, logger)
            logger.info('client held offsets: {}'.format(client.consumer.held_offsets))

        now = int(time.time())
        if last_usn_load_time + 600 <= now :
            mem_size_before = get_memory_usage()
            usn_info_list, distro_pkg_usn_map = load_usns_from_index(args, logger)
            logger.info('loaded security info from index')
            last_usn_load_time = now
            mem_size_after = get_memory_usage()
            mem_diff = mem_size_after - mem_size_before 
            load_count = load_count + 1
            if mem_diff:
                 logger.info('index loading growth load_count={}, growth={}, new_process_size={}'.format(load_count, mem_diff, mem_size_after))

        if usn_info_list is None or distro_pkg_usn_map is None:
            logger.error('error getting security notices from index. Exiting')
            sys.exit(1)

        namespace = None
        timestamp = None
        image_name = None
        container_name = None
        uuid = None
        cur_mem_size = get_memory_usage()
        if cur_mem_size - last_mem_size > 0:
              logger.info('non-process memory growth={}, images processed={}, process_size={}'.format((cur_mem_size - last_mem_size),count, cur_mem_size))
              last_mem_size = cur_mem_size

        try:
            packages = []
            osinfo = None
            data = None
            try:
                data = client.next_frame()
            except  Exception as e:
                logger.warn('exception while fetching data from broker: retry after {} sec sleep:{} '.format(RETRY_INTERVAL, str(e)))
                logger.exception(e) 
                time.sleep(RETRY_INTERVAL)
                try:
                    client.stop()
                except Exception  as e1:
                    pass
                client=None
                continue
            
            stream = StringIO(data)

            csv.field_size_limit(sys.maxsize) # required to handle large value strings
            csv_reader = csv.reader(stream, delimiter='\t', quotechar="'")
            metadata = None
            try:
                for ftype, fkey, fvalue in csv_reader:
                    if ftype == 'package':
                        packages.append(json.loads(fvalue))
                    if ftype == 'os':
                        osinfo=json.loads(fvalue)
                    if not metadata and ftype == 'metadata':
                        metadata = json.loads(fvalue)
            except ValueError, e:
                if metadata:
                    uuid = uuid = metadata.get('uuid', None)
                    logger.warn('uuid={}, reason={}'.format(uuid, str(e)))
                else:
                    data_length = len(data)
                    if data_length > 512: data_length = 512
                    logger.warn('cvs reader could not extract metadata, reason={}, data={}'.format(str(e), data[0:data_length]))
                    stream.close()
                    continue
            stream.close()
            
            features = metadata.get('features',None)
            namespace = metadata.get('namespace', None)
            uuid = metadata.get('uuid', None)
            container_name = metadata.get('container_name', None)
            timestamp = metadata.get('timestamp', None)

            if features and 'configparam' in features:
                # vulnerability annotator does not process configparser data
                continue

            if not all([features, namespace, uuid, container_name, timestamp]):
                logger.error('cannot process as missing one of: features, namespace, uuid, container_name, timestamp from metadata={}'.format(metadata))
                continue

            notification_msg = { 
                'uuid': uuid,
                'processor': PROCESSOR_GROUP,
                'instance-id': args.instance_id,
                'status': 'start',
                'namespace': namespace,
                'timestamp': datetime.datetime.now().isoformat()+'Z',
                'timestamp_ms': int(time.time() * 1000)
            }

            mem_size_before = get_memory_usage()
            msg = json.dumps(notification_msg)
            logger.info(msg)
            client.notify(msg, uuid)
            vulnerabilities = annotator.makeScanForNamespace(namespace, timestamp, osinfo, packages, usn_info_list, distro_pkg_usn_map, uuid, logger)
    
            if vulnerabilities:
                # send metadata for notification purposes by the indexer
                msg_buf = StringIO()                 
                msg_buf.write(json.dumps(metadata))
                msg_buf.write('\n')
                for v in vulnerabilities:
                    #client.publish(json.dumps(v))
                    msg_buf.write(json.dumps(v))
                    msg_buf.write('\n')
                logger.info(json.dumps(vulnerabilities[-1]))
                client.publish(msg_buf.getvalue(), uuid)
                msg_buf = None
                
            notification_msg['status'] = 'completed'
            notification_msg['timestamp'] = datetime.datetime.now().isoformat()+'Z'
            notification_msg['timestamp_ms'] = int(time.time() * 1000)

            msg = json.dumps(notification_msg)
            client.notify(msg, uuid)
            logger.info(msg)
            count = count + 1
            mem_size_after = get_memory_usage()
            mem_diff = mem_size_after - mem_size_before 
            if mem_diff:
                 logger.info('frame handling process growth uuid={}, images_processed={}, growth={}, new_process_size={}'.format(uuid, count, mem_diff, mem_size_after))
        except Exception, e:
            logger.error("Uncaught exception: %s" % e)
            logger.exception(e)
            notification_msg = { 
                'uuid': uuid,
                'processor': PROCESSOR_GROUP,
                'instance-id': args.instance_id,
                'status': 'error',
                'namespace': namespace,
                'timestamp': datetime.datetime.now().isoformat(),
                'timestamp_ms': time.time() * 1000,
                'text': ''.format(e)
            }
            msg = json.dumps(notification_msg)
            client.notify(msg, uuid)

def load_usns_from_index(args, logger):
    while True:
        try:
            usn_info_list = elastic_index.Index(elastic_host=args.elasticsearch_url).get_all_usn()
            distro_pkg_usn_map = usndb.process_usn_info(usn_info_list, logger)
            logger.info('loaded usns from index: number of usn={}'.format(len(usn_info_list)))
            return usn_info_list, distro_pkg_usn_map
        except elasticsearch.NotFoundError, e:
            # immediately after initialization, before usncrawler populates index 
            # ubuntu security notices don't exist
            logger.info(str(e))
            time.sleep(5)
            logger.info("retrying after 5 sec")
        except elasticsearch.ConnectionError, e:
            logger.info(str(e))
            time.sleep(10)
            logger.info("retrying after 10 sec")
        except Exception, e:
            logger.info(str(e))
            logger.info("retrying after 10 sec")
            time.sleep(10)

    return None, None

if __name__ == '__main__':
    log_dir = os.path.dirname(logger_file)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        os.chmod(log_dir,0755)

    format = '%(asctime)s %(levelname)s %(lineno)s  %(funcName)s: %(message)s'
    formatter = logging.Formatter(format)
    
    logger = logging.getLogger("vulnerability-annotator")
    logger.setLevel(logging.INFO)
    
    fh = logging.handlers.RotatingFileHandler(logger_file, maxBytes=2<<27, backupCount=4)
    formatter = logging.Formatter(format)
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    h = NullHandler()
    logging.getLogger("kafka").addHandler(h)
    logging.getLogger("pykafka").addHandler(h)

    try:
        parser = argparse.ArgumentParser(description="")
        parser.add_argument('--kafka-url',  type=str, required=True, help='kafka url: host:port')
        parser.add_argument('--kafka-zookeeper-port',  type=str, required=True, help='kafka zookeeper port')
        parser.add_argument('--receive-topic', type=str, required=True, help='receive-topic')
        parser.add_argument('--notification-topic', type=str, required=True, help='topic to send process notification')
        parser.add_argument('--annotation-topic', type=str, required=True, help='topic to send annotations')
        parser.add_argument('--elasticsearch-url',  type=str, required=True, help='elastic-search url: host:port')
        parser.add_argument('--instance-id',  type=str, required=True, help='instance id')
        args = parser.parse_args()

        process_message(args, logger)

    except Exception, e:
        print('Error: %s' % str(e))
        logger.exception(e) 


