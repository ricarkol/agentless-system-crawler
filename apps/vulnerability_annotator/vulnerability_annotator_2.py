#!/usr/bin/env python

from __future__ import print_function
import logging
import logging.handlers
import time
import signal
import sys
import argparse
import datetime
import csv
import os
import  elasticsearch 
from cStringIO import StringIO
import elastic_index
import resource

try:
    import simplejson as json
except:
    import json
    
import timeout
import kafka as kafka_python
import pykafka
from va_python_base.KafkaInterface import KafkaInterface
import frame_annotator_2
import usndb

logger_file = "/var/log/cloudsight/vulnerability_annotator.log"
PROCESSOR_GROUP = "vuln_annotator"
RETRY_INTERVAL = 60 + 5 # zookeper timeout 60 sec + 5 more

class NullHandler(logging.Handler):
    def emit(self, record):
        pass

def get_memory_usage():
    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024

def sigterm_handler(signum=None, frame=None):
    logger.error('Received SIGTERM signal. Goodbye!')
    sys.exit(0)

signal.signal(signal.SIGTERM, sigterm_handler)

count = 0 # number of frames processed
load_count = 0


def process_message(args, logger):
    
    global count, load_count

    annotator = frame_annotator_2.MakeScan()
    last_usn_load_time = 0
    usn_info_list, distro_pkg_usn_map = None, None
    last_mem_size = get_memory_usage()

    client = None
    while True:
        try:
            client = KafkaInterface(args.kafka_url, args.kafka_zookeeper_port, logger, args.receive_topic, args.annotation_topic, args.notification_topic, PROCESSOR_GROUP)
            break
        except Exception as e:
            logger.error('Failed to establish connection to kafka broker at %s: %s' % \
                        (args.kafka_url, repr(e)))
            time.sleep(5)
    if not client:
        return

    while True:
        try:
            for data in client.next_frame():

                now = int(time.time())
                if last_usn_load_time + 600 <= now :
                    mem_size_before = get_memory_usage()
                    usn_info_list, distro_pkg_usn_map = load_usns_from_index(args, logger)
                    logger.info('loaded security info from index')
                    last_usn_load_time = now
                    mem_size_after = get_memory_usage()
                    mem_diff = mem_size_after - mem_size_before
                    load_count = load_count + 1
                    if mem_diff:
                         logger.info('index loading growth load_count={}, growth={}, new_process_size={}'.format(load_count, mem_diff, mem_size_after))

                if usn_info_list is None or distro_pkg_usn_map is None:
                    logger.error('error getting security notices from index. Exiting')
                    sys.exit(1)

                namespace = None
                timestamp = None
                image_name = None
                container_name = None
                uuid = None
                cur_mem_size = get_memory_usage()
                if cur_mem_size - last_mem_size > 0:
                      logger.info('non-process memory growth={}, images processed={}, process_size={}'.format((cur_mem_size - last_mem_size),count, cur_mem_size))
                      last_mem_size = cur_mem_size


                packages = []
                osinfo = None

                stream = StringIO(data)

                csv.field_size_limit(sys.maxsize) # required to handle large value strings
                csv_reader = csv.reader(stream, delimiter='\t', quotechar="'")
                metadata = None
                try:
                    for ftype, fkey, fvalue in csv_reader:
                        if ftype == 'package':
                            packages.append(json.loads(fvalue))
                        if ftype == 'os':
                            osinfo=json.loads(fvalue)
                        if not metadata and ftype == 'metadata':
                            metadata = json.loads(fvalue)
                except ValueError, e:
                    if metadata:
                        uuid = uuid = metadata.get('uuid', None)
                        logger.warn('uuid={}, reason={}'.format(uuid, str(e)))
                    else:
                        data_length = len(data)
                        if data_length > 512: data_length = 512
                        logger.warn('cvs reader could not extract metadata, reason={}, data={}'.format(str(e), data[0:data_length]))
                        stream.close()
                        continue
                stream.close()

                features = metadata.get('features',None)
                namespace = metadata.get('namespace', None)
                uuid = metadata.get('uuid', None)
                container_name = metadata.get('container_name', None)
                timestamp = metadata.get('timestamp', None)

                if features and 'configparam' in features:
                    # vulnerability annotator does not process configparser data
                    continue

                if not all([features, namespace, uuid, container_name, timestamp]):
                    logger.error('cannot process as missing one of: features, namespace, uuid, container_name, timestamp from metadata={}'.format(metadata))
                    continue

                notification_msg = {
                    'uuid': uuid,
                    'processor': PROCESSOR_GROUP,
                    'instance-id': args.instance_id,
                    'status': 'start',
                    'namespace': namespace,
                    'timestamp': datetime.datetime.now().isoformat()+'Z',
                    'timestamp_ms': int(time.time() * 1000)
                }

                mem_size_before = get_memory_usage()
                msg = json.dumps(notification_msg)
                logger.info(msg)
                client.notify(msg, uuid, namespace)
                vulnerabilities = annotator.makeScanForNamespace(namespace, timestamp, osinfo, packages, usn_info_list, distro_pkg_usn_map, uuid, logger)

                if vulnerabilities:
                    # send metadata for notification purposes by the indexer
                    msg_buf = StringIO()
                    msg_buf.write(json.dumps(metadata))
                    msg_buf.write('\n')
                    for v in vulnerabilities:
                        #client.publish(json.dumps(v))
                        msg_buf.write(json.dumps(v))
                        msg_buf.write('\n')
                    logger.info(json.dumps(vulnerabilities[-1]))
                    client.publish(msg_buf.getvalue(), uuid, namespace)
                    msg_buf = None

                notification_msg['status'] = 'completed'
                notification_msg['timestamp'] = datetime.datetime.now().isoformat()+'Z'
                notification_msg['timestamp_ms'] = int(time.time() * 1000)

                msg = json.dumps(notification_msg)
                client.notify(msg, uuid, namespace)
                logger.info(msg)
                count = count + 1
                # mem_size_after = get_memory_usage()
                # mem_diff = mem_size_after - mem_size_before
                # if mem_diff:
                #      logger.info('frame handling process growth uuid={}, images_processed={}, growth={}, new_process_size={}'.format(uuid, count, mem_diff, mem_size_after))

        except Exception, e:
            logger.error("Exiting with exception: %s" % repr(e))
            logger.exception(e)
            raise e


def load_usns_from_index(args, logger):
    while True:
        try:
            usn_info_list = elastic_index.Index(elastic_host=args.elasticsearch_url).get_all_usn()
            distro_pkg_usn_map = usndb.process_usn_info(usn_info_list, logger)
            logger.info('loaded usns from index: number of usn={}'.format(len(usn_info_list)))
            return usn_info_list, distro_pkg_usn_map
        except elasticsearch.NotFoundError, e:
            # immediately after initialization, before usncrawler populates index 
            # ubuntu security notices don't exist
            logger.info(str(e))
            time.sleep(5)
            logger.info("retrying after 5 sec")
        except elasticsearch.ConnectionError, e:
            logger.info(str(e))
            time.sleep(10)
            logger.info("retrying after 10 sec")
        except Exception, e:
            logger.info(str(e))
            logger.info("retrying after 10 sec")
            time.sleep(10)

    return None, None

if __name__ == '__main__':
    log_dir = os.path.dirname(logger_file)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        os.chmod(log_dir,0755)

    format = '%(asctime)s %(levelname)s %(lineno)s %(funcName)s: %(message)s'
    logging.basicConfig(format=format, level=logging.INFO)
    logger = logging.getLogger("vulnerability-annotator")
    fh = logging.handlers.RotatingFileHandler(logger_file, maxBytes=2<<27, backupCount=4)
    formatter = logging.Formatter(format)
    fh.setFormatter(formatter)
    logger.addHandler(fh)


    try:
        parser = argparse.ArgumentParser(description="")
        parser.add_argument('--kafka-url',  type=str, required=True, help='kafka url: host:port')
        parser.add_argument('--kafka-zookeeper-port',  type=str, required=True, help='kafka zookeeper port')
        parser.add_argument('--receive-topic', type=str, required=True, help='receive-topic')
        parser.add_argument('--notification-topic', type=str, required=True, help='topic to send process notification')
        parser.add_argument('--annotation-topic', type=str, required=True, help='topic to send annotations')
        parser.add_argument('--elasticsearch-url',  type=str, required=True, help='elastic-search url: host:port')
        parser.add_argument('--instance-id',  type=str, required=True, help='instance id')
        args = parser.parse_args()

        process_message(args, logger)

    except Exception, e:
        print('Error: %s' % str(e))
        logger.exception(e)
        raise e


